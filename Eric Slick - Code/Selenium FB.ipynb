{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########## Main Function - Retrieves WebElements and stores them into the database ##########\n",
    "########## Note: Imports must be called outside of the function, and exist above the function on the server. ##########\n",
    "def main_scrape(brand, date_r):\n",
    "    ########## Instantiation of the element holding variables and \"list_holder\" to keep track of when the date was exceeded ##########\n",
    "    likes = []\n",
    "    dates = []\n",
    "    comments = []\n",
    "    posts = []\n",
    "    list_holder = 0\n",
    "    ########## Chrome options to make the code work on the server ##########\n",
    "    chromeOptions = Options()\n",
    "    chromeOptions.add_argument('--no-sandbox')\n",
    "    chromeOptions.add_argument('--remote-debugging-port=44224')\n",
    "    chromeOptions.add_argument('--disable-dev-shm-using')\n",
    "    chromeOptions.add_argument('--headless')\n",
    "    chromeOptions.add_argument('disable-setuid-sandbox')\n",
    "    chromeOptions.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=chromeOptions)\n",
    "    \n",
    "    \n",
    "    date_range = eval_dates(date_r)\n",
    "    driver.get('https://www.facebook.com/{}'.format(brand))\n",
    "\n",
    "    ########## ESTABLISH CONNECTION WITH DB ##########\n",
    "    # server = \"ls-1ef1825172e62dcc237ee491d09a0c12aff562fe.cn5ycdfnko6g.us-east-1.rds.amazonaws.com\"\n",
    "    # database_ = 'smcDB'\n",
    "    # username = 'dbmasteruser'\n",
    "    # password_ = 'q+o.H1sd$CRRZl&CSl>VK}-(~+t1ea&P'\n",
    "\n",
    "    # db = pymysql.connect(host=server, user=username, password=password_, database=database_, charset='utf8mb4',\n",
    "    #                      cursorclass=pymysql.cursors.DictCursor, port=3306)\n",
    "    # cursor = db.cursor()\n",
    "\n",
    "    # print(\"Database connection successfully established\")\n",
    "\n",
    "\n",
    "    # While loop that continues to scroll, grab elements, and modify them accordingly until the date range is reached.\n",
    "    count = 0\n",
    "    while True:\n",
    "        rand = random.randint(0, 10000)\n",
    "        count += 1\n",
    "        likes = driver.find_elements_by_xpath('/html/body/div[1]/div[3]/div[1]/div/div/div[2]/div[2]/div/div[3]/div[2]/div/div[1]/div/div[2]/div/div[1]/div[2]/div/div/div[2]/div[2]/form/div/div[2]/div[1]/div/div[1]/a/span[2]/span/span')\n",
    "        #likes = driver.find_elements_by_class_name(\"_81hb\")\n",
    "        dates = driver.find_elements_by_class_name(\"timestampContent\")\n",
    "        comments = driver.find_elements_by_class_name(\"_3hg-._42ft\")\n",
    "        posts = driver.find_elements_by_class_name(\"_5pbx.userContent._3576\")\n",
    "        shares = driver.find_elements_by_class_name(\"_355t._4vn2\")\n",
    "        ########## Print the data - for testing purposes ##########\n",
    "        #print_data(posts, likes, dates, comments)\n",
    "        print(likes)\n",
    "\n",
    "\n",
    "########## Code for the database, commented until server issues are fixed with element grabbing ##########\n",
    "#         sql = '''\n",
    "#         insert into smcDB.FACEBOOK(BrandHandle,PostUrl,PostDate,PostText,Likes,Comments,Shares) values('%s','%s',\n",
    "#         '%s','%s','%s','%s','%s') \n",
    "#         ''' % (handle, rand, dates, posts, likes, comments, shares)\n",
    "#         cursor.execute(sql)\n",
    "#         db.commit()\n",
    "\n",
    "#         print(\"Successfully committed to database\")\n",
    "\n",
    "        # Remove duplicate likes from the list\n",
    "        likes = remove_dupes(likes)\n",
    "        # Standardize the date values for evaluation\n",
    "        dates = eval_dates(dates)\n",
    "        # Check if the date is outside of the specified range\n",
    "        done_searching, list_holder = cancel_scraping(date_range, dates)\n",
    "\n",
    "        # If date is outside range, return values\n",
    "        if done_searching:\n",
    "            print(\"Date range exceeded, terminating.\")\n",
    "            return posts, likes, comments, shares, dates\n",
    "        # Scroll \"infinitely\" or until we find a date outside the range\n",
    "        scroller(driver)\n",
    "        rand_time = random.randint(25,60)\n",
    "        time.sleep(rand_time)\n",
    "\n",
    "        # Specify how many scrolls we want to make (used mostly for dates that would be considered long ago)\n",
    "        if count == 2:\n",
    "            return posts, likes, comments, shares, dates\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    #remove_extras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scroll down the screen and load new elements\n",
    "def scroller(driver):\n",
    "    curr_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, \" + str(curr_height) + \");\")\n",
    "        time.sleep(1\n",
    "                   )\n",
    "        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        if curr_height == new_height:\n",
    "            break\n",
    "        curr_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print out the data retrieved to the console - mostly for testing\n",
    "def print_data(posts, likes, dates, comments):\n",
    "    for item in posts:\n",
    "        print(\"Posts: \",item.text)\n",
    "    for item in likes:\n",
    "        print(\"Likes: \",item)\n",
    "    for item in comments:\n",
    "        print(\"Comments: \",item.text)\n",
    "    for item in dates:\n",
    "        print(\"Dates:\",item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove duplicate values from the element lists\n",
    "def remove_dupes(likes):\n",
    "    temp = []\n",
    "    for item in likes:\n",
    "        if item.text not in temp:\n",
    "            temp.append(item.text)\n",
    "        else:\n",
    "            continue   \n",
    "    return temp    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize dates, can take in a list or a single string value\n",
    "def eval_dates(dates):\n",
    "    if isinstance(dates, list):\n",
    "        temp = []\n",
    "        temp_ = []\n",
    "        for item in dates:\n",
    "            temp.append(parse(item.text))\n",
    "        for item in temp:\n",
    "            temp_.append(item.date())\n",
    "        return temp_\n",
    "    elif isinstance(dates, str):\n",
    "            temp_str = parse(dates)\n",
    "            temp_str = temp_str.date()\n",
    "            return temp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cancel scraping based on the date input from the form\n",
    "def cancel_scraping(date, dates):\n",
    "    count = 0\n",
    "    for item in dates:\n",
    "        count += 1\n",
    "        print(item, date)\n",
    "        if item < date:\n",
    "            return True, count\n",
    "        else:\n",
    "            continue\n",
    "    return False, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove extra values from lists, due to nature of element pulling, even when the date is\n",
    "# outside of the range it gets added because they're pulled in chunks. This will remove extras.\n",
    "def remove_extras(list_holder, dates):\n",
    "    for i in range(list_holder-1, len(dates)):\n",
    "        print(\"Index:\", i)\n",
    "        print(\"Length:\", len(dates))\n",
    "        print(\"List Holder: \", list_holder)\n",
    "        del posts[i]\n",
    "        del likes[i]\n",
    "        del comments[i]\n",
    "        del shares[i]\n",
    "        del dates[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Code Testing Section #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from selenium import webdriver\n",
    "    from selenium.common.exceptions import TimeoutException\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    import getpass\n",
    "    import calendar\n",
    "    import os\n",
    "    import platform\n",
    "    import sys\n",
    "    import time\n",
    "    import urllib.request\n",
    "    import pymysql\n",
    "    import random\n",
    "    from dateutil.parser import parse\n",
    "    import datetime\n",
    "posts, likes, comments, shares, dates = main_scrape(\"cocacolaunitedstates\", \"2021-06-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Documentation Section #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Regarding Functionality on Server #####\n",
    "'''\n",
    "1.) As it stands - the Facebook scraper behaves correctly on a windows machine. However, when the scraper is run on the AWS \n",
    "instance it automatically is redirected to the login page. Currently I've attempted the following fixes:\n",
    "    - Rotating Proxies\n",
    "    - Disabling Selenium headers\n",
    "    - Redirecting from the login page to the page that is to be scraped\n",
    "    - Mobile emulation\n",
    "Come semester 2, I have plans to attempt the following fixes:\n",
    "    - Modify the Selenium source code such that Facebook cannot tell from Seleniums own Javascript it's being scraped.\n",
    "    - Attempt to route the requests through the Selenium Hub onto a windows machine, and run the scraping from the windows\n",
    "      operating system.\n",
    "      \n",
    "2.) Obviously, if the application is not able to scrape on the server, it is not functional. However, if the issue of \n",
    "automatic redirection to a login page can be overcome, the code will work as intended with no modifications.\n",
    "\n",
    "3.) If it is not possible to find a solution in the second semester, I plan to attempt the following solutions:\n",
    "    - Scrape the HTML using only BeautifulSoup by gathering all of the HTML once and parsing it offline on the server.\n",
    "    - Logging in. It should be noted, logging in is seen as a last resort as the client does not want to have to login \n",
    "      under any circumstances if it can be avoided. Additionally, logging in is risky because it opens the server to \n",
    "      potentially being IP banned.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
